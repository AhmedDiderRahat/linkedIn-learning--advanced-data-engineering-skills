---
title: LinkedIn Learning- Advanced Data Engineering Skills
author: Rahat A.D.
date: September 10, 2023
output:
  github_document:
    toc: true
    toc_depth: 2
---

# Introduction

Welcome to the repository for my advanced data engineering courses! This collection of courses has provided me with valuable skills and knowledge in the field of data engineering, covering a wide range of topics including data warehousing, data processing with PySpark, and workflow automation with Apache Airflow.

In this repository, you will find summaries and key takeaways from each of the courses, along with any additional resources or code examples that I have found helpful during my learning journey. Whether you're just starting to explore the world of data engineering or looking to expand your existing skills, I hope you find these course summaries and insights valuable.



## Course 1: Data Science on Google Cloud Platform: Designing Data Warehouses

- Skills Gains: 
	1. Datawarehouse use case
	2. Google cloud storage fundamentals
	3. Google cloud SQL
	4. Google Big-Query
	5. Create, Update, and Delete tables
	6. Filtering
	7. Grouping and aggregation
	8. Joins and sub-query
	9. Partioning and external tables
	10. Table design consideration
	11. Optimize storage
	12. Monitoring and logging



## Course 2: Apache PySpark by Example

- Skills Gains: 
	1. Why PySpark?
	2. Apache PySpark Eco-System
	3. Responsibilites of different elements of the Eco-System
	4. Spark Component
	5. Different types of cluster manager
	6. PySpark Operaetions: View, collect, limit, print_schema, filter, sort, join, cache

- [Hand-Note Link](https://github.com/AhmedDiderRahat/linkedIn-learning--advanced-data-engineering-skills/blob/main/Course-2-Apache%20PySpark%20by%20Example/Note/Apache%20PySpark%20Example.pdf) 

- [Code-Note Link](https://github.com/AhmedDiderRahat/linkedIn-learning--advanced-data-engineering-skills/tree/main/Course-2-Apache%20PySpark%20by%20Example/code)



## Course 3: Learning Apache Airflow

- Skills Gains: 
	1. Data Engineering workflow
	2. Airflow fundamentals
	3. Airflow architecture
	4. Airflow use-case
	5. Opeartors: Bash, Python, SQLite
	6. Xcoms
	7. Airflow user creatiopn and role distribution
	8. Setup airflow on windox using wsl
	9. Execute branching
	10. Bash scriping
	11. Execute Python-pipeline
	12. Execute SQL-pipeline
	13. Execute multiple tasks
	14. Cross-task communication 
	15. Task Scheduling 
	16. Cron catchup
	
- [Note Link](https://github.com/AhmedDiderRahat/linkedIn-learning--advanced-data-engineering-skills/blob/main/Course-3-Learning%20Apache%20Airflow/Note/Course%E2%80%942-Handnote.pdf)
- [Code Link](https://github.com/AhmedDiderRahat/linkedIn-learning--advanced-data-engineering-skills/tree/main/Course-3-Learning%20Apache%20Airflow/Code/airflow)



# Conclusion

Completing these advanced data engineering courses has been an enriching experience, equipping me with essential skills and knowledge in various aspects of data engineering. From designing data warehouses on Google Cloud Platform to harnessing the power of Apache PySpark for data processing and orchestrating complex workflows with Apache Airflow, these courses have expanded my horizons and enabled me to tackle real-world data engineering challenges.

As I continue to explore and apply these skills in practical scenarios, I am excited about the opportunities that lie ahead in the ever-evolving field of data engineering. I hope that the course summaries and insights shared in this repository prove to be valuable to others on their learning journeys.

Thank you for joining me in this exploration of advanced data engineering concepts, and I look forward to continued growth and collaboration in the data engineering community.

Feel free to connect with me on LinkedIn or GitHub if you have any questions or would like to discuss data engineering topics further. Happy learning!
