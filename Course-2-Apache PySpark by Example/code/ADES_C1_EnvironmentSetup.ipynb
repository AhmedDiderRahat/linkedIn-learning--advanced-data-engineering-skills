{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests tqdm\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\DELL',\n",
       " 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Temp\\\\spark-0cef82a8-0822-4c6e-b58c-b5603f43bcdd\\\\userFiles-2acffa45-3776-41ea-a849-1c81d9a2ee96',\n",
       " 'C:\\\\anaconda\\\\python311.zip',\n",
       " 'C:\\\\anaconda\\\\DLLs',\n",
       " 'C:\\\\anaconda\\\\Lib',\n",
       " 'C:\\\\anaconda',\n",
       " '',\n",
       " 'C:\\\\anaconda\\\\Lib\\\\site-packages',\n",
       " 'C:\\\\anaconda\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\anaconda\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\anaconda\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-UI2D542:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cd7cb396d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pyspark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 1.73GB [22:50, 1.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD'\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = 1024  # You can adjust the chunk size as needed.\n",
    "    \n",
    "    with open('reported-crimes.csv', 'wb') as file, tqdm(\n",
    "        desc='Downloading',\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=chunk_size):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+----+\n",
      "|      ID|Case Number|               Date|IUCR|\n",
      "+--------+-----------+-------------------+----+\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|0810|\n",
      "|11645836|   JC212333|2016-05-01 00:25:00|1153|\n",
      "|11243268|   JB167760|2017-01-01 00:01:00|1562|\n",
      "| 1896258|    G749215|2001-12-15 02:00:00|0460|\n",
      "|11645527|   JC212744|2015-02-02 10:00:00|1153|\n",
      "+--------+-----------+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read first 5 ROWS\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp,col,lit\n",
    "df = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n",
    "\n",
    "df.select('ID', 'Case Number', 'Date', 'IUCR').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('ID', StringType(), True), StructField('Case Number', StringType(), True), StructField('Date', TimestampType(), True), StructField('Block', StringType(), True), StructField('IUCR', StringType(), True), StructField('Primary Type', StringType(), True), StructField('Description', StringType(), True), StructField('Location Description', StringType(), True), StructField('Arrest', StringType(), True), StructField('Domestic', BooleanType(), True), StructField('Beat', StringType(), True), StructField('Ward', StringType(), True), StructField('Community Area', StringType(), True), StructField('FBI Code', StringType(), True), StructField('X Coordinate', StringType(), True), StructField('Y Coordinate', StringType(), True), StructField('Year', IntegerType(), True), StructField('Updated On', TimestampType(), True), StructField('Latitude', DoubleType(), True), StructField('Longitude', DoubleType(), True), StructField('Location', StringType(), True)])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read with schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, DoubleType, IntegerType\n",
    "\n",
    "labels = [\n",
    "    ('ID', StringType()),\n",
    "    ('Case Number', StringType()),\n",
    "    ('Date', TimestampType()),\n",
    "    ('Block', StringType()),\n",
    "    ('IUCR', StringType()),\n",
    "    ('Primary Type', StringType()),\n",
    "    ('Description', StringType()),\n",
    "    ('Location Description', StringType()),\n",
    "    ('Arrest', StringType()),\n",
    "    ('Domestic', BooleanType()),\n",
    "    ('Beat', StringType()),\n",
    "    ('Ward', StringType()),\n",
    "    ('Community Area', StringType()),\n",
    "    ('FBI Code', StringType()),\n",
    "    ('X Coordinate', StringType()),\n",
    "    ('Y Coordinate', StringType()),\n",
    "    ('Year', IntegerType()),\n",
    "    ('Updated On', TimestampType()),\n",
    "    ('Latitude', DoubleType()),\n",
    "    ('Longitude', DoubleType()),\n",
    "    ('Location', StringType())\n",
    "]\n",
    "\n",
    "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: string (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: timestamp (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read with schema\n",
    "\n",
    "df = spark.read.csv('reported-crimes.csv', schema=schema, header=True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----+----+\n",
      "|      ID|Case Number|Date|IUCR|\n",
      "+--------+-----------+----+----+\n",
      "|11646166|   JC213529|null|0810|\n",
      "|11645836|   JC212333|null|1153|\n",
      "|11243268|   JB167760|null|1562|\n",
      "| 1896258|    G749215|null|0460|\n",
      "|11645527|   JC212744|null|1153|\n",
      "+--------+-----------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ID', 'Case Number', 'Date', 'IUCR').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----+----+-------+\n",
      "|      ID|Case Number|Date|IUCR|   year|\n",
      "+--------+-----------+----+----+-------+\n",
      "|10224881|   HY411873|null|1310|1875669|\n",
      "|10225155|   HY412177|null|0470|1926785|\n",
      "|10225206|   HY412253|null|041A|1873609|\n",
      "|10225234|   HY412306|null|0420|1901570|\n",
      "|10225308|   HY412341|null|2230|1924028|\n",
      "+--------+-----------+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter data \n",
    "df1 = df.filter(col('Year') >= 2022)\n",
    "df1.select('ID', 'Case Number', 'Date', 'IUCR', 'year').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   year|\n",
      "+-------+\n",
      "|1862793|\n",
      "|1873451|\n",
      "|1932760|\n",
      "|1917570|\n",
      "|1832283|\n",
      "|1840869|\n",
      "|1843917|\n",
      "|1916160|\n",
      "|1909736|\n",
      "|1836937|\n",
      "|1883862|\n",
      "|1899958|\n",
      "|1859559|\n",
      "|1874811|\n",
      "|1858095|\n",
      "|1848641|\n",
      "|1891458|\n",
      "|1905902|\n",
      "|1919587|\n",
      "|1850347|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique data\n",
    "\n",
    "df.select('year').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   year|\n",
      "+-------+\n",
      "|   null|\n",
      "|      0|\n",
      "|1813894|\n",
      "|1813896|\n",
      "|1813897|\n",
      "|1813908|\n",
      "|1813909|\n",
      "|1813910|\n",
      "|1813911|\n",
      "|1813913|\n",
      "|1813914|\n",
      "|1813917|\n",
      "|1813919|\n",
      "|1813925|\n",
      "|1813930|\n",
      "|1813931|\n",
      "|1813932|\n",
      "|1813937|\n",
      "|1813938|\n",
      "|1813939|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sorting\n",
    "df.select('year').distinct().orderBy(col('year')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7878531"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of the rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|       Primary Type|  count|\n",
      "+-------------------+-------+\n",
      "|              THEFT|1662584|\n",
      "|            BATTERY|1439063|\n",
      "|    CRIMINAL DAMAGE| 898212|\n",
      "|          NARCOTICS| 749291|\n",
      "|            ASSAULT| 515453|\n",
      "|      OTHER OFFENSE| 489070|\n",
      "|           BURGLARY| 426930|\n",
      "|MOTOR VEHICLE THEFT| 385705|\n",
      "| DECEPTIVE PRACTICE| 351261|\n",
      "|            ROBBERY| 296074|\n",
      "+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouped by counts\n",
    "df.groupBy('Primary Type').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: what percentage of reported crimes resulted in an arrest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|Arrest|  count|Percentage|\n",
      "+------+-------+----------+\n",
      "| false|5832714|     74.03|\n",
      "|  true|2045817|     25.97|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get groupwise counts\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Assuming df is your Spark DataFrame\n",
    "grouped_df = df.groupBy('Arrest').count()\n",
    "\n",
    "total_count = df.count()\n",
    "\n",
    "ans = grouped_df.withColumn(\n",
    "    'Percentage',\n",
    "    round(col('count') / total_count * 100, 2).cast(\"double\")\n",
    ")\n",
    "\n",
    "ans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: 25.97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find top-3 location for reported crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|Location Description|  count|\n",
      "+--------------------+-------+\n",
      "|              STREET|2054941|\n",
      "|           RESIDENCE|1317288|\n",
      "|           APARTMENT| 896269|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Location Description').count().orderBy('count', ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045817"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col('Arrest') == True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'PythonEvalType', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Union', 'UserDefinedFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aggregate', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_length', 'bitwiseNOT', 'bitwise_not', 'broadcast', 'bround', 'bucket', 'call_udf', 'cast', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'filter', 'first', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'hour', 'hours', 'hypot', 'initcap', 'inline', 'inline_outer', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'nanvl', 'next_day', 'np', 'nth_value', 'ntile', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'percent_rank', 'percentile_approx', 'pmod', 'posexplode', 'posexplode_outer', 'pow', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_remote_functions', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'unwrap_udt', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'window_time', 'xxhash64', 'year', 'years', 'zip_with']\n"
     ]
    }
   ],
   "source": [
    "# see all function\n",
    "from pyspark.sql import functions\n",
    "print(dir(functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The position is not zero based, but 1 based index.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    str : :class:`~pyspark.sql.Column` or str\n",
      "        target column to work on.\n",
      "    pos : int\n",
      "        starting position in str.\n",
      "    len : int\n",
      "        length of chars.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        substring of given value.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 4 char of a string\n",
    "from pyspark.sql.functions import lower, upper, substring\n",
    "\n",
    "# get the documention \n",
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+\n",
      "|lower(Primary Type)|upper(Primary Type)|PT-4|\n",
      "+-------------------+-------------------+----+\n",
      "|              theft|              THEFT|THEF|\n",
      "| deceptive practice| DECEPTIVE PRACTICE|DECE|\n",
      "|        sex offense|        SEX OFFENSE|SEX |\n",
      "|            battery|            BATTERY|BATT|\n",
      "| deceptive practice| DECEPTIVE PRACTICE|DECE|\n",
      "+-------------------+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the lower, upper, and first 4 characters of string\n",
    "df.select(lower(col('Primary Type')), upper(col('Primary Type')), substring(col('Primary Type'), 1, 4).alias('PT-4')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|2018-09-01 00:01:00|\n",
      "|2016-05-01 00:25:00|\n",
      "|2017-01-01 00:01:00|\n",
      "|2001-12-15 02:00:00|\n",
      "|2015-02-02 10:00:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the oldest date and most recent date\n",
    "from pyspark.sql.functions import min, max, to_timestamp, col, lit\n",
    "\n",
    "df = spark.read.csv('reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n",
    "\n",
    "df.select('Date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|         Oldes Date|        Recent Date|\n",
      "+-------------------+-------------------+\n",
      "|2001-01-01 00:00:00|2018-11-11 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Date')).distinct().select(min(col('Date')).alias('Oldes Date'), max(col('Date')).alias('Recent Date')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------------------+----+\n",
      "|ID      |Case Number|Date                  |IUCR|\n",
      "+--------+-----------+----------------------+----+\n",
      "|11646166|JC213529   |09/01/2018 12:01:00 AM|0810|\n",
      "|11645836|JC212333   |05/01/2016 12:25:00 AM|1153|\n",
      "|11243268|JB167760   |01/01/2017 12:01:00 AM|1562|\n",
      "|1896258 |G749215    |12/15/2001 02:00:00 AM|0460|\n",
      "|11645527|JC212744   |02/02/2015 10:00:00 AM|1153|\n",
      "+--------+-----------+----------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = spark.read.csv('reported-crimes.csv', header=True)\n",
    "\n",
    "new_df.select('ID', 'Case Number', 'Date', 'IUCR').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
